{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "276be6eb-33e2-42d8-8861-199d8e803480",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Obtaining dependency information for pandas from https://files.pythonhosted.org/packages/e3/59/35a2892bf09ded9c1bf3804461efe772836a5261ef5dfb4e264ce813ff99/pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/jessica/.pyenv/versions/3.10.6/envs/movie_genre_prediction/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas)\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /home/jessica/.pyenv/versions/3.10.6/envs/movie_genre_prediction/lib/python3.10/site-packages (from pandas) (1.25.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/jessica/.pyenv/versions/3.10.6/envs/movie_genre_prediction/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.0.3 pytz-2023.3 tzdata-2023.3\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56cea406-c362-4ab7-a892-c57cafda578b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Obtaining dependency information for numpy from https://files.pythonhosted.org/packages/71/3c/3b1981c6a1986adc9ee7db760c0c34ea5b14ac3da9ecfcf1ea2a4ec6c398/numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Downloading numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "Successfully installed numpy-1.25.2\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9af6719f-1e94-450d-9b56-655521099b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Using cached seaborn-0.12.2-py3-none-any.whl (293 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in /home/jessica/.pyenv/versions/3.10.6/envs/movie_genre_prediction/lib/python3.10/site-packages (from seaborn) (1.25.2)\n",
      "Requirement already satisfied: pandas>=0.25 in /home/jessica/.pyenv/versions/3.10.6/envs/movie_genre_prediction/lib/python3.10/site-packages (from seaborn) (2.0.3)\n",
      "Collecting matplotlib!=3.6.1,>=3.1 (from seaborn)\n",
      "  Obtaining dependency information for matplotlib!=3.6.1,>=3.1 from https://files.pythonhosted.org/packages/c2/da/a5622266952ab05dc3995d77689cba600e49ea9d6c51d469c077695cb719/matplotlib-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading matplotlib-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib!=3.6.1,>=3.1->seaborn)\n",
      "  Obtaining dependency information for contourpy>=1.0.1 from https://files.pythonhosted.org/packages/aa/55/02c6d24804592b862b38a85c9b3283edc245081390a520ccd11697b6b24f/contourpy-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading contourpy-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.7 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib!=3.6.1,>=3.1->seaborn)\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib!=3.6.1,>=3.1->seaborn)\n",
      "  Obtaining dependency information for fonttools>=4.22.0 from https://files.pythonhosted.org/packages/87/61/f50ab3237b0cbf2b0be12274227f912d30f94e2b93fb8bae92c91107eee8/fonttools-4.42.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading fonttools-4.42.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (150 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.6/150.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1 (from matplotlib!=3.6.1,>=3.1->seaborn)\n",
      "  Using cached kiwisolver-1.4.4-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/jessica/.pyenv/versions/3.10.6/envs/movie_genre_prediction/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (23.1)\n",
      "Collecting pillow>=6.2.0 (from matplotlib!=3.6.1,>=3.1->seaborn)\n",
      "  Obtaining dependency information for pillow>=6.2.0 from https://files.pythonhosted.org/packages/3d/36/e78f09d510354977e10102dd811e928666021d9c451e05df962d56477772/Pillow-10.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata\n",
      "  Downloading Pillow-10.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.5 kB)\n",
      "Collecting pyparsing<3.1,>=2.3.1 (from matplotlib!=3.6.1,>=3.1->seaborn)\n",
      "  Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/jessica/.pyenv/versions/3.10.6/envs/movie_genre_prediction/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/jessica/.pyenv/versions/3.10.6/envs/movie_genre_prediction/lib/python3.10/site-packages (from pandas>=0.25->seaborn) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/jessica/.pyenv/versions/3.10.6/envs/movie_genre_prediction/lib/python3.10/site-packages (from pandas>=0.25->seaborn) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/jessica/.pyenv/versions/3.10.6/envs/movie_genre_prediction/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
      "Using cached matplotlib-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "Using cached contourpy-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
      "Downloading fonttools-4.42.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached Pillow-10.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib, seaborn\n",
      "Successfully installed contourpy-1.1.0 cycler-0.11.0 fonttools-4.42.0 kiwisolver-1.4.4 matplotlib-3.7.2 pillow-10.0.0 pyparsing-3.0.9 seaborn-0.12.2\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38bb12d6-268b-4685-aae0-bf322613f8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "from warnings import warn\n",
    "from time import sleep\n",
    "from random import randint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64a09c95-2976-494b-860f-3222958cc09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mystery\n",
    "URL1 = 'https://www.imdb.com/search/title/?title_type=feature&num_votes=10000,&genres=mystery&languages=en&sort=user_rating,desc&start='\n",
    "URL2 = '&explore=genres&ref_=adv_nxt'\n",
    "\n",
    "# Romance\n",
    "URL3 = 'https://www.imdb.com/search/title/?title_type=feature&num_votes=10000,&genres=romance&languages=en&sort=user_rating,desc&start='\n",
    "\n",
    "# Sci-Fi\n",
    "URL4 = 'https://www.imdb.com/search/title/?title_type=feature&num_votes=10000,&genres=sci-fi&languages=en&sort=user_rating,desc&start='\n",
    "\n",
    "# Sport\n",
    "URL5 = 'https://www.imdb.com/search/title/?title_type=feature&num_votes=10000,&genres=sport&languages=en&sort=user_rating,desc&start='\n",
    "\n",
    "# Thriller\n",
    "URL6 = 'https://www.imdb.com/search/title/?title_type=feature&num_votes=10000,&genres=thriller&languages=en&sort=user_rating,desc&start='\n",
    "\n",
    "# War\n",
    "URL7 = 'https://www.imdb.com/search/title/?title_type=feature&num_votes=10000,&genres=war&languages=en&sort=user_rating,desc&start='\n",
    "\n",
    "# Western\n",
    "URL8 = 'https://www.imdb.com/search/title/?title_type=feature&num_votes=10000,&genres=western&languages=en&sort=user_rating,desc&start='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6622fb-9db3-4bce-b0d2-13e4db6cd677",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = [1,51,101,151,201]\n",
    "headers = {'Accept-Language': 'en-US,en;q=0.8'} # If this is not specified, the default language is Mandarin\n",
    "\n",
    "#initialize empty lists to store the variables scraped\n",
    "mystery_titles = []\n",
    "mystery_imdb_ids = []\n",
    "mystery_genres = []\n",
    "mystery_imgdata = []\n",
    "\n",
    "for page in pages:\n",
    "  \n",
    "   #get request for sci-fi\n",
    "   response = get(URL1\n",
    "                  + str(page)\n",
    "                  + URL2, headers=headers)\n",
    "  \n",
    "   sleep(randint(8,15))\n",
    "   \n",
    "   #throw warning for status codes that are not 200\n",
    "   if response.status_code != 200:\n",
    "       warn('Request: {}; Status code: {}'.format(requests, response.status_code))\n",
    "\n",
    "   #parse the content of current iteration of request\n",
    "   page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "      \n",
    "   movie_containers = page_html.find_all('div', class_ = 'lister-item mode-advanced')\n",
    "  \n",
    "   #extract the 50 movies for that page\n",
    "   for container in movie_containers:\n",
    "\n",
    "        #title\n",
    "        title = container.h3.a.text\n",
    "        mystery_titles.append(title)\n",
    "\n",
    "        #imdb_id\n",
    "        imdb_id = container.find('a')['href'].strip().split('/')[-2]\n",
    "        mystery_imdb_ids.append(imdb_id)\n",
    "\n",
    "        #images\n",
    "        imageDiv = container.find('div', class_ = 'lister-item-image float-left')\n",
    "        img = imageDiv.img['loadlate']\n",
    "        mystery_imgdata.append(img)\n",
    "            \n",
    "        #genre\n",
    "        genre = container.p.find('span', class_ = 'genre').text.replace(\"\\n\", \"\").rstrip().split(',') # remove the whitespace character, strip, and split to create an array of genres\n",
    "        mystery_genres.append(genre)\n",
    "\n",
    "mystery_df = pd.DataFrame({'movie': mystery_titles,\n",
    "                      'imdb_id': mystery_imdb_ids,\n",
    "                      'genre': mystery_genres,\n",
    "                          'image_url': mystery_imgdata})\n",
    "mystery_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f039930-b1c7-4aac-b5ca-32227d2fa114",
   "metadata": {},
   "outputs": [],
   "source": [
    "mystery_imgdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fdf0c4-cbd1-4b38-a946-e7c8ad906db5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pages = [1,51,101,151,201]\n",
    "headers = {'Accept-Language': 'en-US,en;q=0.8'} # If this is not specified, the default language is Mandarin\n",
    "\n",
    "#initialize empty lists to store the variables scraped\n",
    "romance_titles = []\n",
    "romance_imdb_ids = []\n",
    "romance_genres = []\n",
    "romance_imgdata = []\n",
    "\n",
    "for page in pages:\n",
    "  \n",
    "   #get request for sci-fi\n",
    "   response = get(URL3\n",
    "                  + str(page)\n",
    "                  + URL2, headers=headers)\n",
    "  \n",
    "   sleep(randint(8,15))\n",
    "   \n",
    "   #throw warning for status codes that are not 200\n",
    "   if response.status_code != 200:\n",
    "       warn('Request: {}; Status code: {}'.format(requests, response.status_code))\n",
    "\n",
    "   #parse the content of current iteration of request\n",
    "   page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "      \n",
    "   movie_containers = page_html.find_all('div', class_ = 'lister-item mode-advanced')\n",
    "  \n",
    "   #extract the 50 movies for that page\n",
    "   for container in movie_containers:\n",
    "\n",
    "        #title\n",
    "        title = container.h3.a.text\n",
    "        romance_titles.append(title)\n",
    "\n",
    "        #imdb_id\n",
    "        imdb_id = container.find('a')['href'].strip().split('/')[-2]\n",
    "        romance_imdb_ids.append(imdb_id)\n",
    "\n",
    "        #images\n",
    "        imageDiv = container.find('div', attrs={'class':'lister-item-image float-left'})\n",
    "        img = imageDiv.img['loadlate']\n",
    "        romance_imgdata.append(img)\n",
    "            \n",
    "        #genre\n",
    "        genre = container.p.find('span', class_ = 'genre').text.replace(\"\\n\", \"\").rstrip().split(',') # remove the whitespace character, strip, and split to create an array of genres\n",
    "        romance_genres.append(genre)\n",
    "\n",
    "romance_df = pd.DataFrame({'movie': romance_titles,\n",
    "                      'imdb_id': romance_imdb_ids,\n",
    "                      'genre': romance_genres,\n",
    "                          'image_url': romance_imgdata}\n",
    "                          )\n",
    "romance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6b2d7f-61a3-44cc-9ccb-8affe7a34dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add webscraping_jess.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adedd78-a2a6-410f-bb5f-3b617c2a49bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!git commit -m \"webscraping start\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bc1fc6-be1f-49d2-b8cd-95167a6a05eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git push origin webscraping_jess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d79dc2-9356-4051-ab13-02a56f138706",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pages = [1,51,101,151,201]\n",
    "headers = {'Accept-Language': 'en-US,en;q=0.8'} # If this is not specified, the default language is Mandarin\n",
    "\n",
    "#initialize empty lists to store the variables scraped\n",
    "scifi_titles = []\n",
    "scifi_imdb_ids = []\n",
    "scifi_genres = []\n",
    "scifi_imgdata = []\n",
    "\n",
    "for page in pages:\n",
    "  \n",
    "   #get request for sci-fi\n",
    "   response = get(URL4\n",
    "                  + str(page)\n",
    "                  + URL2, headers=headers)\n",
    "  \n",
    "   sleep(randint(8,15))\n",
    "   \n",
    "   #throw warning for status codes that are not 200\n",
    "   if response.status_code != 200:\n",
    "       warn('Request: {}; Status code: {}'.format(requests, response.status_code))\n",
    "\n",
    "   #parse the content of current iteration of request\n",
    "   page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "      \n",
    "   movie_containers = page_html.find_all('div', class_ = 'lister-item mode-advanced')\n",
    "  \n",
    "   #extract the 50 movies for that page\n",
    "   for container in movie_containers:\n",
    "\n",
    "        #title\n",
    "        title = container.h3.a.text\n",
    "        scifi_titles.append(title)\n",
    "\n",
    "        #imdb_id\n",
    "        imdb_id = container.find('a')['href'].strip().split('/')[-2]\n",
    "        scifi_imdb_ids.append(imdb_id)\n",
    "\n",
    "        #images\n",
    "        imageDiv = container.find('div', attrs={'class':'lister-item-image float-left'})\n",
    "        img = imageDiv.img['loadlate']\n",
    "        scifi_imgdata.append(img)\n",
    "            \n",
    "        #genre\n",
    "        genre = container.p.find('span', class_ = 'genre').text.replace(\"\\n\", \"\").rstrip().split(',') # remove the whitespace character, strip, and split to create an array of genres\n",
    "        scifi_genres.append(genre)\n",
    "\n",
    "scifi_df = pd.DataFrame({'movie': scifi_titles,\n",
    "                      'imdb_id': scifi_imdb_ids,\n",
    "                      'genre': scifi_genres,\n",
    "                        'image_url': scifi_imgdata}\n",
    "                          )\n",
    "scifi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220fb07c-4f32-48df-996f-74d74cd884de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pages = [1,51,101,151,201]\n",
    "headers = {'Accept-Language': 'en-US,en;q=0.8'} # If this is not specified, the default language is Mandarin\n",
    "\n",
    "#initialize empty lists to store the variables scraped\n",
    "sport_titles = []\n",
    "sport_imdb_ids = []\n",
    "sport_genres = []\n",
    "sport_imgdata = []\n",
    "\n",
    "for page in pages:\n",
    "  \n",
    "   #get request for sci-fi\n",
    "   response = get(URL5\n",
    "                  + str(page)\n",
    "                  + URL2, headers=headers)\n",
    "  \n",
    "   sleep(randint(8,15))\n",
    "   \n",
    "   #throw warning for status codes that are not 200\n",
    "   if response.status_code != 200:\n",
    "       warn('Request: {}; Status code: {}'.format(requests, response.status_code))\n",
    "\n",
    "   #parse the content of current iteration of request\n",
    "   page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "      \n",
    "   movie_containers = page_html.find_all('div', class_ = 'lister-item mode-advanced')\n",
    "  \n",
    "   #extract the 50 movies for that page\n",
    "   for container in movie_containers:\n",
    "\n",
    "        #title\n",
    "        title = container.h3.a.text\n",
    "        sport_titles.append(title)\n",
    "\n",
    "        #imdb_id\n",
    "        imdb_id = container.find('a')['href'].strip().split('/')[-2]\n",
    "        sport_imdb_ids.append(imdb_id)\n",
    "\n",
    "        #images\n",
    "        imageDiv = container.find('div', attrs={'class':'lister-item-image float-left'})\n",
    "        img = imageDiv.img['loadlate']\n",
    "        sport_imgdata.append(img)\n",
    "            \n",
    "        #genre\n",
    "        genre = container.p.find('span', class_ = 'genre').text.replace(\"\\n\", \"\").rstrip().split(',') # remove the whitespace character, strip, and split to create an array of genres\n",
    "        sport_genres.append(genre)\n",
    "\n",
    "sport_df = pd.DataFrame({'movie': sport_titles,\n",
    "                      'imdb_id': sport_imdb_ids,\n",
    "                      'genre': sport_genres,\n",
    "                        'image_url': sport_imgdata}\n",
    "                          )\n",
    "sport_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01413884-caf0-49c6-953e-b007de141477",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = [1,51,101,151,201]\n",
    "headers = {'Accept-Language': 'en-US,en;q=0.8'} # If this is not specified, the default language is Mandarin\n",
    "\n",
    "#initialize empty lists to store the variables scraped\n",
    "thriller_titles = []\n",
    "thriller_imdb_ids = []\n",
    "thriller_genres = []\n",
    "thriller_imgdata = []\n",
    "\n",
    "for page in pages:\n",
    "  \n",
    "   #get request for sci-fi\n",
    "   response = get(URL6\n",
    "                  + str(page)\n",
    "                  + URL2, headers=headers)\n",
    "  \n",
    "   sleep(randint(8,15))\n",
    "   \n",
    "   #throw warning for status codes that are not 200\n",
    "   if response.status_code != 200:\n",
    "       warn('Request: {}; Status code: {}'.format(requests, response.status_code))\n",
    "\n",
    "   #parse the content of current iteration of request\n",
    "   page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "      \n",
    "   movie_containers = page_html.find_all('div', class_ = 'lister-item mode-advanced')\n",
    "  \n",
    "   #extract the 50 movies for that page\n",
    "   for container in movie_containers:\n",
    "\n",
    "        #title\n",
    "        title = container.h3.a.text\n",
    "        thriller_titles.append(title)\n",
    "\n",
    "        #imdb_id\n",
    "        imdb_id = container.find('a')['href'].strip().split('/')[-2]\n",
    "        thriller_imdb_ids.append(imdb_id)\n",
    "\n",
    "        #images\n",
    "        imageDiv = container.find('div', attrs={'class':'lister-item-image float-left'})\n",
    "        img = imageDiv.img['loadlate']\n",
    "        thriller_imgdata.append(img)\n",
    "            \n",
    "        #genre\n",
    "        genre = container.p.find('span', class_ = 'genre').text.replace(\"\\n\", \"\").rstrip().split(',') # remove the whitespace character, strip, and split to create an array of genres\n",
    "        thriller_genres.append(genre)\n",
    "\n",
    "thriller_df = pd.DataFrame({'movie': thriller_titles,\n",
    "                      'imdb_id': thriller_imdb_ids,\n",
    "                      'genre': thriller_genres,\n",
    "                           'image_url': thriller_imgdata}\n",
    "                          )\n",
    "thriller_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b4dcac-719c-4c29-bbb6-5c9bf6191745",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = [1,51,101,151,201]\n",
    "headers = {'Accept-Language': 'en-US,en;q=0.8'} # If this is not specified, the default language is Mandarin\n",
    "\n",
    "#initialize empty lists to store the variables scraped\n",
    "war_titles = []\n",
    "war_imdb_ids = []\n",
    "war_genres = []\n",
    "war_imgdata = []\n",
    "\n",
    "for page in pages:\n",
    "  \n",
    "   #get request for sci-fi\n",
    "   response = get(URL7\n",
    "                  + str(page)\n",
    "                  + URL2, headers=headers)\n",
    "  \n",
    "   sleep(randint(8,15))\n",
    "   \n",
    "   #throw warning for status codes that are not 200\n",
    "   if response.status_code != 200:\n",
    "       warn('Request: {}; Status code: {}'.format(requests, response.status_code))\n",
    "\n",
    "   #parse the content of current iteration of request\n",
    "   page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "      \n",
    "   movie_containers = page_html.find_all('div', class_ = 'lister-item mode-advanced')\n",
    "  \n",
    "   #extract the 50 movies for that page\n",
    "   for container in movie_containers:\n",
    "\n",
    "        #title\n",
    "        title = container.h3.a.text\n",
    "        war_titles.append(title)\n",
    "\n",
    "        #imdb_id\n",
    "        imdb_id = container.find('a')['href'].strip().split('/')[-2]\n",
    "        war_imdb_ids.append(imdb_id)\n",
    "\n",
    "        #images\n",
    "        imageDiv = container.find('div', attrs={'class':'lister-item-image float-left'})\n",
    "        img = imageDiv.img['loadlate']\n",
    "        war_imgdata.append(img)\n",
    "            \n",
    "        #genre\n",
    "        genre = container.p.find('span', class_ = 'genre').text.replace(\"\\n\", \"\").rstrip().split(',') # remove the whitespace character, strip, and split to create an array of genres\n",
    "        war_genres.append(genre)\n",
    "\n",
    "war_df = pd.DataFrame({'movie': war_titles,\n",
    "                      'imdb_id': war_imdb_ids,\n",
    "                      'genre': war_genres,\n",
    "                      'image_url': war_imgdata}\n",
    "                          )\n",
    "war_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8baac21-1421-4726-9101-aa1de02259e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = [1,51,101,151,201]\n",
    "headers = {'Accept-Language': 'en-US,en;q=0.8'} # If this is not specified, the default language is Mandarin\n",
    "\n",
    "#initialize empty lists to store the variables scraped\n",
    "western_titles = []\n",
    "western_imdb_ids = []\n",
    "western_genres = []\n",
    "western_imgdata = []\n",
    "\n",
    "for page in pages:\n",
    "  \n",
    "   #get request for sci-fi\n",
    "   response = get(URL8\n",
    "                  + str(page)\n",
    "                  + URL2, headers=headers)\n",
    "  \n",
    "   sleep(randint(8,15))\n",
    "   \n",
    "   #throw warning for status codes that are not 200\n",
    "   if response.status_code != 200:\n",
    "       warn('Request: {}; Status code: {}'.format(requests, response.status_code))\n",
    "\n",
    "   #parse the content of current iteration of request\n",
    "   page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "      \n",
    "   movie_containers = page_html.find_all('div', class_ = 'lister-item mode-advanced')\n",
    "  \n",
    "   #extract the 50 movies for that page\n",
    "   for container in movie_containers:\n",
    "\n",
    "        #title\n",
    "        title = container.h3.a.text\n",
    "        western_titles.append(title)\n",
    "\n",
    "        #imdb_id\n",
    "        imdb_id = container.find('a')['href'].strip().split('/')[-2]\n",
    "        western_imdb_ids.append(imdb_id)\n",
    "\n",
    "        #images\n",
    "        imageDiv = container.find('div', class_ = 'lister-item-image float-left')\n",
    "        img = imageDiv.img['loadlate']\n",
    "        western_imgdata.append(img)\n",
    "            \n",
    "        #genre\n",
    "        genre = container.p.find('span', class_ = 'genre').text.replace(\"\\n\", \"\").rstrip().split(',') # remove the whitespace character, strip, and split to create an array of genres\n",
    "        western_genres.append(genre)\n",
    "\n",
    "western_df = pd.DataFrame({'movie': western_titles,\n",
    "                      'imdb_id': western_imdb_ids,\n",
    "                      'genre': western_genres,\n",
    "                          'image_url': western_imgdata})\n",
    "western_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
